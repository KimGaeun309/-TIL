# 1주차 녹강 - overview, 부품, 성능, 파워, 멀티프로세서

## 1주 1차시 - 컴퓨터 구조 오버뷰

### 컴퓨터 구조 오버뷰

CAU01-intro\ComputerArchitectureOverview(1p)

컴퓨터 구조에 대한 전체적인 개관과 컴퓨터 시스템이 어떤 기술에 의해 발전했는지, 그리고 컴퓨터는 내부에 어떤 부품들을 가지고 있는지, 컴퓨터 성능, 전력과 멀티프로세서에 대해 살펴보자

- The Computer Revolution 컴퓨터 시스템 발전
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/427d1986-b279-4e64-bdea-d9f26d20700f/Untitled.png)
    
    컴퓨터 시스템 발전의 추세를 무어의 법칙으로 많이 얘기한다. 무어의 법칙이란 1.5년 또는 2년 내에 반도체 집적도가 2배가 되며 성능, 기능이 2배가 되어 기하 급수적으로 발전해왔다는 것을 의미한다. 자동차가 컴퓨터와 같은 속도로 발전했더라면 런던부터 뉴욕의 거리를 1초만에 갈 수 있었을 것이다. (컴퓨터는 매우 빠르게 발전했다) 따라서 컴퓨터와 관련된 여러 응용 분야가 있다. 자동차의 경우 테슬라 내부에 컴퓨터가 들어가 있다. 또 자율주행의 기술도 발전하고 있다. 또 Personal Mobile Devices 또한 급격하게 발전하였다. World Wide Web으로 다양한 정보들이 쌓이고 Search Engine 이 동작하며 정보를 모아오고 검색을 할 때 많은 정보들을 순식간에 모아올 수 있다. 여기에 통신 기술도 들어갈 수 있을 것이다. 무어의 법칙 그래프를 보면 일년에 52%씩, 한 주에 1%씩 기술이 발전하던 시기도 있었다. 그리고 최근엔 새롭게 빅데이터, 인공지능, ,클라우드, IoT, 로봇, 게임, 메타버스 등이 발전하였다. 
    
- Classes of Computers
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b6148089-5cb0-4663-b327-3dda5bb03c17/Untitled.png)
    
    Personal computers → 다양한 용도로 사용할 수 있음. 가성비가 중요.
    
    Server computers → 고성능이고 용량이 큰 컴퓨터. 원격으로 액세스. 신뢰성이 높아야 한다. 24시간 내내 동작할 수 있어야 함.
    
    Super computers → 과학이나 군사 목적으로 대단위의 계산을 할 필요가 있을 때 사용. 대표적으로 기상 예측을 위한 슈퍼 컴퓨터가 있다. (각 지역의 기압, 온도 등 정보를 수집해 미래의 날씨를 예측하기 위한 계산을 함) 
    
    Embedded computers → 특정 제품에 들어가 있는 컴퓨터 시스템. 도처에 널려 있음. IoT가 바로 이런 것. 기능적인 일을 하거나 계산을 하는데 컴퓨터가들어감.
    
    PostPC Era - PC 이후에 나온 다양한 형태의 장치들 - Personal Moile Device (Smartpone, Tablet, Smart glass 등), 네트워크 에지 디바이스 (Wearable - Smart watch 등), 클라우드 컴퓨팅 (VDI(Virtual Desktop Infrastructure로 모니터와 서버에 연결할 수 있는 장치만 가지고 서버에 연결해 사용하는 것.), SaaS(Software as a Service로 클라우드에서 소프트웨어 서비스 사용하는 것. ex - 오피스 365))
    
- Old & New Computers
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7a602fc3-555b-4f2f-b694-ba183c366b0a/Untitled.png)
    
    ENIAC → 최초의 전자식 컴퓨터. 전선들이 있어서 사람이 일일히 전선의 연결을 새로 해서 새로운 프로그램을 실행하는 형태였음.
    
    EDVAC → 최초의 프로그램 내장형 컴퓨터. 
    
    Apple 2 → 터미널 형태의 프로그램과 인터랙트. 커맨드 라인 형태로 명령을 내림. 작은 테잎에 데이터를 저장하고 로딩해서 사용함.
    
    Smartphone, Smart watch, TPU Cloud, Google Datacenter(안에 컨테이너가 엄청 많고 그 안에 컴퓨터가 있다. WSC) → 현대의 컴퓨터들.
    
- What You Will Learn
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/392d73de-eb17-4dce-aad4-90844f311c16/Untitled.png)
    
    컴퓨터 아키텍쳐에 대해 배울 것이다. = Computer Design. → 컴퓨터 하드웨어의 요소들은 무엇이 있는지, 명령어 집합, 데이터 표현, 입출력 매커니즘, 메모리 어드레싱 등과 같은 전반적인 내용을 다룰 것이다.
    
    또 컴퓨터 오거나이제이션에 대해 배울 것이다. → 설계된 것들이 어떤 식으로 하드웨어가 구현되어 있고 어떻게 서로 연결되어 있는지, 디테일, 시그널들, 인터페이스, 메모리 등의 내용을 다룰 것이다.
    
    어떻게 프로그램이 기계어로 번역되는가? 또 그것을 어떻게 하드웨어가 실행하는가?
    
    하드웨어와 소프트웨어의 인터페이스 - 명령어 집합, 시스템 컨트롤 인터페이스
    
    프로그램  돌릴 때의 성능은 무엇에 의해 결정되며, 우리가 어떻게 해야 성능이 높아질까?
    
    컴퓨터의 성능을 높이기 위한 방법으로써의 병렬 처리
    
- Six Great Ideas in Computer Architecture
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3fb0d4f2-22f6-4a55-acc4-877d15dd560e/Untitled.png)
    
    컴퓨터 발전에서의 6가지 요소  **끊김**
    
- Computer Layers - Typical Computer
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/84018a4a-aaff-4365-8975-d11d53c92a67/Untitled.png)
    
    컴퓨터 추상화에 대한 이야기를 하자. 노란 선 위로 소프트웨어이다. 소프트웨어는 application software, system software로 분류가 된다. 시스템 소프트웨어는 컴파일러를 가지고 있고, 윈도우, 맥OS 등과 같은 운열체제를 뜻한다. application software를 짤 때 많은 것들이 라이브러리에 있고 보다 큰 레벨의 컴포넌트가 있다. 노란 선 밑에 하드웨어에는 processor, memory 등이 있는데 이들은 회로 level (datapath & control)로 구성된다. 논리 회로를 가지고 만들어진다. 논리회로는 Gate level에서 구성된다. Gate는 반도체인 트랜지터로 만든다. 이것이 Circuit Design level이다. 트랜지터는 반도체 공장에서 인쇄 기술을 이용해 그림을 그림으로써 만들어진다. 이것이 Layout level이다. 우리가 모든 것을 다 알지 않아도 되는 것이 abstracion에 의한 효과이다. 
    
- Abstraction
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/50a6ea5a-d6c2-4deb-bb47-bc813dd1d40c/Untitled.png)
    
    추상화에 대해 더 구체적인 내용을 살펴보자. 
    
    c언어로 프로그램을 짜면 컴파일러를 이용해 어셈블리어로 바꾼다. 어셈블리어는 assembler를 이용해 컴퓨터가 사용하는 기계어로 바꿀 수 있다. 이 기계어가 하드웨어에서 동작해야 한다. 이때 하드웨어에서 가장 중추적인 것이 cpu이고 cpu의 가장 중요한 부품이 ALU (Arithmetic Logic Unit, 산술 연산과 논리 연산을 하는 하드웨어 모듈)와 Register File (ALU가 동작하기 위해 필요한 데이터가 저장되는 곳)이다. 이 ALU 등이 구현될 때는 논리 회로로 구현될 것이다. 
    
- Instructions Codes
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5833f314-e8bc-4968-82bb-10dea93db88e/Untitled.png)
    
    컴퓨터의 동작에 대해 더 살펴보자. 0과 1로 구성된 코드인  instruction code는 프로그램에 사용되는데 프로그램이란 이러한 instruction들의 sequence이다. 실제로 프로그램은 명령어 코드와 데이터로 구성되어 있다. 또 이런 것들은 하드디스크 등에 프로그램 형태로 저장이 되어 있고 우리가 운영체제에게 이 프로그램을 돌리고 싶다고 명령을 주면 이 안에 있는 명령어들이 메모리의 instruction 부분에 올라오고 데이터들이 메모리의 data 부분에 올라온다. 또 프로그램이 동작하면서 사용하는 데이터들이 만들어지기도 한다. 이러한 프로그램을 실행시키기 위해 프로세서가 동작을 하게 되는데 프로세서는 프로그램 카운터라는 레지스터를 가지고 있다. 이 프로그램 카운터는 실행해야 될 명령어들의 첫 번째 주소를 가지고 있다. 이 주소를 가지고 첫 번째 명령어를 하나 가져온다. 이 명령어가 컨트롤 유닛으로 오게 되면 이게 무슨 명령어이고 어떤 데이터를 사용하며 결과를 어디에 저장하는지에 대한 정보를 추출해 data path를 제어해 레지스터와 ALU를 사용해 동작시킨다. (load: 메모리→레지스터, store: 레지스터→메모리)
    
- Computer Structure - Modern View
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/06b7685e-d8b5-4d7a-ac2a-c0a951b32073/Untitled.png)
    
    현대적인 컴퓨터 구조를 보자. 
    
    컨테이너들이 있고 컨테이너 내부에 서버 렉?이 쭉 진열되어있다. 서버 렉에는 서버 컴퓨터들이 16개씩 들어있다. 이 16개의 컴퓨터들을 보면 그 안에 보드 형식으로 구성이 되어 있다. 이 보드 형식의 모양을 자세히 보면 프로세서를 여러 개 가지고 있다. ( 프로세서 안을 보면 코어가 또 여러 개 있다. ) 그리고 메인 메모리가 있고 입출력 장치가 있다. 그리고 프로세서 코어를 자세히 살펴보면 그 안에 명령어들을 처리하는 instruction unit이 있고 functional unit이 있고 cache 라는 빠른 메모리가 있다. fuctional unit을 자세히 보면 다시 logic gates 레벨에서 구현이 되어 있을 것이다. 
    
    병렬 처리(parallelism)을 사용하고 있다. 어떤 식으로 병렬 처리가 이루어질까? 
    
    1. Parallel Request: 데이터 센터와 같은 warehouse 에 검색 요청이 병렬적으로 들어오면 그 처리를 여러 서버에 분산시켜서 병렬적으로 처리하도록 한다. 
    2. Parallel Threads: 각 서버 내의 프로세서가 여러 개의 코어를 가지고 있기 때문에 쓰레드를 병렬적으로 사용할 수 있다. 한 쓰레드가 한 cpu 안에서 동작을 할 때 pipeline이나 super scalar 등과 같은 parallel 명령어 처리 방식을 활용해 동작하도록 되어 있다. 
    3. Parallel Data: 데이터들 자체가 parallel하게 동작할 수 있다.  function unit이 더하기를 동시에 네 개를 진행할 수 있다. 즉 하나의 명령어에 대해 네 개의 데이터를 처리할 수 있는 SIMD와 같은 명령어들이 동작할 수 있다.
    4. Hardware descriptions: 게이트들을 보면 시그널들이 병렬적으로 동작하도록 되어 있다. 
- Moore’s Law
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d2f02f10-52bf-4154-b67f-8ead5bec57d9/Untitled.png)
    
    무어 → 1년마다 코어의 트랜지스터 개수가 2.5배가 될 것이라고 예측함.
    
    실제로 그렇게 계속 발전하고 있음.
    
    용량이 엄청나게 커짐. 성능이 많이 좋아짐.
    
- Principle of Locality/Memory Hierarchy
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9365004a-d97e-49cc-a481-0ec0671b6d5f/Untitled.png)
    
    메모리 계층구조. cpu 안에 레지스터가 있다. 이것은 cpu가 빠르게 접근 가능한 데이터이다. 그리고 Main memory가 있다. 그런데 원래는 프로세스(cpu)의 속도와 메인 메모리의 속도가 비슷했는데 프로세스가 빠르게 발전하면서 performance gap이 발생해 속도가 느려 캐시를 사람들이 고안해 냄. Cache는 적은 용량의 sram?을 사용해 속도가 굉장히 빠르고 비싸다. 적은 용량을 임시로 가져다 놓고 cpu가 빠르게 접근할 수 있도록 했다. 캐시 레벨 1, 2, 3로 나뉘게 되었다. 그리고 데이터를 영구히 보존하기 위해 디스크를 사용하는데 디스크에 모든 것을 저장하여 사용하도록 되어 있다. 하지만 이 디스크가 느리기 때문에 SSD를 우리가 사용하게 되었다.  그리고 그 밑 레벨에서는 테이프 형태로 저장되어 있는 데이터 스토리지들이 있다. 이들은 백업용이다. 계층에서 위로 갈수록 비싼 대신 속도가 빠르다. 
    
- Dependability via Redundancy
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2dd57231-28d2-4620-b581-44c450b4f902/Untitled.png)
    
    redundancy에 의한 dependability. 
    
    안전하게 동작해야 하는 시스템들이 있다 그러한 시스템들은 dependability가 높은, 신뢰성이 높은 시스템이어야 한다. 그럴 때 redundancy(중복) 사용. 예를 들어 세 개의 모듈에 같은 연산을 시켰는데 두 개는 같은 대답을 하고 하나는 다른 대답을 하면 두 모듈이 답한 값이 정확하다고 판단할 수 있다. 이런 것은 데이터 센터에서 많이 사용한다. 데이터 센터가 고장나면 서비스가 막히고 큰일이 날 수 있기 때문이다. 그리고 RAID (Redundant Arrays of Indepedent Disks)는 싼 디스크 여러 개를 사용해 중복해서 사용해 데이터를 저장해 안전하게 데이터를 유지하겠다는 것이다. 그리고 고성능 서버 급의 메모리는 ECC Memory가 들어간다. 이는 데이터를 저장하는 메모리에 부가적으로 더 추가해서 메모리를 가지고 있다. 그래서 데이터 몇 개가 잘못되어도 복구할 수 있다. 
    

## 1주 2차시 - 컴퓨터 부품과 성능

### 컴퓨터 부품

CAU01-intro\ComputerComponents(8p)

컴퓨터 부품에 대해 알아보자. 

- A Computer System
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4196abb0-4a4a-426d-b4f1-f24f928a5fa4/Untitled.png)
    
    컴퓨터 시스템에 대한 간단한 블럭 다이어그램이다. 
    
    여기서 프로세서란 컴퓨터에서 가장 중요한 메인 역할을 하는 것으로 cpu로도 불린다. 예전에는 mpu라는 말도 썼다.  
    
    그리고 또 중요한 것으로 메인 메모리가 있다. 메인 메모리는 DRAM으로 만들어지는데 DRAM은 다이내믹 램이다. 
    
    그리고 캐시가 들어가는데 캐시는 SRAM, 스태틱 램이다.
    
    그리고 프로세서와 메모리가 연결되려면 버스 형태로 만들어지는데 bus 는 여러 시스템들이 쉽게 엮을 수 있도록 만들어진 규약이다. PCIE 등 여러 형태의 버스가 이다. USB 또한 유니버셜 시리얼 버스이다. 메모리와 입출력 장치와 프로세서를 연결하는 버스가 있다.
    
    그리고 다음으로 입출력을 제어하는 입출력 컨트롤러가 있다. 이를 통해 그래픽 출력이 나갈 수 있는데 이 두 개가 합쳐지면 그래픽 카드가 될 수 있다. 입출력 컨트롤러는 네트워크와 연결될 수도 있고 키보드나 마우스 등과 연결될 수도 있다. 그리고 디스크와 연결되어 디스크 컨트롤러가 될 수도 있다.
    
- Components of a Computer
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3f112162-e71b-4248-9c4c-94efe50136ed/Untitled.png)
    
    실제적인 컴퓨터 컴포넌트 살펴보기. 컴퓨터는 cpu와 메모리 장치와 입출력 장치들로 이루어져 있는데 cpu 내부에는 레지스터가 있고 ALU를 비롯한 combitional logic이 있고 컨트롤 유닛이 있다. 그리고 메인 메모리와 연결되어 작동하며 입력 장치, 외부 장치가 있다. 입력 장치는 외부의 데이터를 내부로 옮기기 위한 장치이고, 출력 장치는 내부의 데이터를 cpu가 처리한 결과를 외부로 보내기 위한 장치이다. 
    
    이러한 구조로 이루어졌기 때문에 데스크탑, 서버, 엠베디드를 포함한 모든 컴퓨터들이 같은 형태의 부품을 가지고 있다. UI 디바이스의 예로써 이스플레이, 키보드, 마우스, 터치스크린이 있고, 스토리지로 ssd, hdd, cd, dvd, sd card가 있고, 네트워크 어댑터(컨트롤 어댑터)로 LAN, Wireless-LAN, Bluetooth가 있다.
    
    아래에는 pc의 메인 보드가 있다. 메인보드에는 cpu, main memory가 있고, 파워가 들어갈 수 있는 곳도 있고, PCI&PCIe Slot 라고 그래픽 카드 등을 꽅을 수 있는 곳이 있다. 또 입출력 관련된 침들이 내장되어 있어 이 칩으로 인해 USB, HDMI, 오디오 등이 입출력될 수 있다. 또 PS2(?) 커넥터가 있는데 예전에는 여기에 마우스와 키보드를 연결했다.
    
    그 옆에는 아이패드를 분해한 모습이 있는데, 디스플레이 껍데기와 실제 디스플레이, 배터리, 메인보드를 확인할 수 있다.
    
- Inside the Processor (Intel Ice Lake)
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b4c377b1-8bb5-4633-96c3-99f024143029/Untitled.png)
    
    프로세서의 내부 구조 확인하자. 인텔 아이스 레이크 cpu의 내부. 코어 네 개를 볼 수 있다. 이것 각각이 cpu의 기능을 한다. 이 코어의 특성으로 ALU가 열 개라 동시에 열 개의 명령어를 처리할 수 있다는 것(10-way superscalar)과 명령어 처리 단계가 14 단계로 나뉘어져 있어 14개의 명령어를 동시에 처리할 수 있다는 것(14 pipeline stages)과 멀티스레딩이 가능해 한 코어 내에서 여러 개의 명령어를 처리할 수 있다는 것(symmetric multithreading)이 있다. 그리고 이러한 코어들이 연결되게 하기 위한 Ring interconnection이 있을 수 있고, 캐시 파트가 있는 것도 확인할 수 있다. 캐시 파트는 이 층 내에서 분리된 L1 캐시 (split L1) 라고 명령어와 데이터를 다르게 보관하는 캐시들이 있고 Unified L2 캐시라고 명령어와 데이터를 같이 가진 캐시들이 있고 Shard Unified L3 캐시가 있다. 그리고 GT는 GPU이다. 인텔 cpu는 그 내부에 그래픽 모듈을 가지고 있어서 그래픽 카드가 없더라도 그래픽을 처리할 수 있게 한다. GPU는 꽤 큰 자리를 차지하고 있다. 그리고 TYPE C 라고 되어 있는 IO port와 관련된 logic들을 보여주고 있고 OPIO라고 on-package io라는 것이 같이 IO를 담당하고 있다. IPU는 image processing unit으로, 카메라에서 들어온 것들의 이미지 처리를 위한 장치이다. 그리고 DISPLAY라고 외부의 디스플레이 장치를 갖다가 드라이브에서 디스플레이 표시할 수 있게 해주는 드라이브 하드웨어가 있다. DDR은 메인 보드를 연결하게 하기 위한 memory interface, 즉 메모리 컨트롤러이다.
    
- Inside the Core
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d7dc6608-12eb-4dd7-8b85-5bd08b2d64dc/Untitled.png)
    
    실제로 코어 안을 보면 그 안에 세 가지 형태로 나눌 수 있는데, Datapath, Control unit, Cache memory가 있다. 캐시 메모리는 SRAM (static ram)으로 만들어져 있으며 속도가 빠른 대신 비싸다. 빠른 메모리를 코어 안에 넣음으로써 데이터를 빠르게 엑세스할 수 있도록 한 것이다. 
    
    마더 보드에 프로세서 칩이 장착되어 있는데 프로세서 칩은 코어들, 캐시들로 구성이 되어있고, 코어를 확대해서 보면 코어에는 데이터패스, 컨트롤유닛, 캐시들이 있다.
    
    발전과정을 보여주는 표를 보면 주황색 점들이 칩 안에 들어가는 트랜지스터의 개수가 시대에 따라 증가하고 있음을 보여준다. 파란색 점들은 하나의 스레드의 성능이 증가하고 있음을 보여주고 있다. 또 초록색 점들은 주파수의 성장을 보여주는데 물리적 한계로 어느 순간부터 성장이 멈춘 것을 볼 수 있다. 빨간 점들은 전력 소모를 보여주는데 주파수의 성장이 멈춘 이후로 같이 성장이 멈춘 것을 확인할 수 있다. 검은 점들은 논리적 코어의 개수를 뜻하는데 최근까지 증가한 것으로 보아 단일 코어의 성능이 정체되더라도 계속 성능이 높아지고 있음을 알 수 있다.
    
- A Safe Place for Data
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dceb6c61-09fb-4d02-af0a-473ca7c59f9f/Untitled.png)
    
    휘발성의 메인 메모리 - 전원을 끄면 다 사라지는 메모리. 그 전형적인 예로 DDR을 들 수 있다. 현재는 DDR4-25600 SDRAM (synchronoss DRAM)을 많이 쓰고 있다. 이는 3200 MHz 에서 동작할 수 있는 것이다. DIMM (Dual Inline Memory Module) 는 DRAM 칩들이 양 쪽에 꽂혀 있는 것을 뜻한다.
    
    휘발하지 않는 메모리 - ‘스토리지’라고 더 많이 불림. 그 대표적인 것으로 HDD(하드 디스크 드라이브)가 있다. 이것이 요즘은 SSD로 바뀌고 있는데, SSD(Solid-State Drive, 반도체 드라이브)는 플래쉬 메모리이고, 플래쉬 메모리의 예로는 SSD 뿐만 아니라 USB Flash Drive, SD card 등이 있다. 그 외에도 Optical disk들이 있는데 그 예로는 CDROM, DVD, BD 등이 있다. 
    
    표를 보면 진화할수록 동작 전압이 낮아지고 동작 주파수가 높아져 전력소모가 감소되고, Die Density(용량) 이 높아지는 것을 볼 수 있다.
    
- Input/Output Devices
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/42fc495e-bc34-4bf9-83e7-a2faf12a10f7/Untitled.png)
    
    입출력 장치들이다. 입력 장치로 키보드, 카메라, 스캐너, 마우스, 마이크, 조이스틱 등이 있고 출력 장치로 프린터, 스피커, 빔 프로젝터 등이 있으며, 입출력 모두 하는 장치로 카메라, 터치스크린 모니터, 팩스, 구글 글래스 등이 있다.
    
- Networks
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4a849b6f-35c0-42e3-8d7c-472f9c7823e6/Untitled.png)
    
    네트워크란 서로 다른 컴퓨터가 통신을 하거나 자원을 공유하거나  nonlocal access를 할 때 사용한다. 네트워크의 예시로 LAN (Local area network), 근거리 네트워크가 있다. 여기서 Ethernet이 동작한다. 그리고 WAN (Wide area network) 가 있는데 이는 외부에서 집에 들어올 때 사용되며 Internet이 동작한다. 그리고 WiFi나 블루투스 등과 같은 Wireless network 도 있다.
    

### 컴퓨터 성능

CAU01-intro\ComputerPerformance(12p)

성능에 대한 것들ㅇ르 살펴보자.

- Defining Performance
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ff8d753-f494-45ca-985c-aeb1f95d1f83/Untitled.png)
    
    다음 중 어떤 비행기가 가장 성능이 좋을까? → 승객을 얼마나 태울 수 있을까? 어느 것이 가장 멀리까지 나갈까? 어느 것이 가장 빠를까? 승객과 속도의 곱은 어느 것이 가장 좋을까? 처럼 성능은 정의가 명확해야 확실하게 비교할 수 있다.
    
- Performance Metrics
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2d542e84-62ab-4247-b8b8-d314f27092c4/Untitled.png)
    
    Response time → 반응 시간, 수행하는데 걸리는 시간을 말한다. 이것이 적을수록 성능이 좋다.
    
    Throughput → 처리량, 단위시간당 몇 개의 일이 처리되는지를 말한다. bandwidth는 데이터가 갈 때 몇 비트씩 운반되는지를 말할 때 쓰는 통신 용어(?). 
    
    만약 프로세서를 더 빠른 것으로 바꾸면 Response time과 Throughput이 얼마나 바뀔까? → Response time이 줄어들 것이고 Throughput도 더 좋아질 것이다. 
    
    프로세서를 더 추가시키면? → Response time은 동일하나 Throughput은 좋아질 것이다.
    
    우리는 현재 cpu 설계에 대해 중점적으로 다루고 있기 때문에 Response time에 집중해 설명할 것이다.
    
    Execution time = Turnaround time = Response time : Processing의 시간, 입출력의 시간, 운영체제 overhead, idle time 등을 다 포함하고 있으며 system performance를 결정한다.
    
    어떤 프로그램을 실행시켰을 때 response time을 살펴보면 순수하게 유저 프로그램이 도는 user cpu time이 있고, 운영체제가 돌기 위한 system cpu time, 입출력에 필요한 i/o time, 쉬는 시간인 idle time 이 있을 수 있다. 그래서 cpu time이란 user cpu time과 system cpu time을 통틀어 말하는 것이다. 
    
    $$
    cpu Time = user Cpu Time + system Cpu Time
    $$
    
- Relative Performance
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/679896eb-e3ca-4fb7-85e3-9222efbc286a/Untitled.png)
    
    시간에 의해 성능이 결정된다고 말했는데 그 중에서도 execution time이 작을수록 좋은 컴퓨터가 되는데 performance 를 어떤 프로그램을 돌렸을 때 걸리는 시간 분의 1로 정의했다. 
    
    $$
    Performance = 1/Execution Time
    $$
    
    X 컴퓨터가 Y  컴퓨터보다 n배 빠르다 → 
    
    $$
    performance_x / performance_y = executionTime_y / executionTime_x = n
    $$
    
    예를 들어 X에서 실행시간이 10초, Y에서 15초라면 15s/10s = 1.5 이므로 X의 수행 시간은 Y보다 1.5배 빠르다.
    
    성능이 더 좋다고 표현할 때 speedup이란 표현을 쓰기도 한다. Y에 비해 X가 speedup이 얼마인가? = Y에 비해 X가 얼마나 빠른가? → 
    
    $$
    speedup Of X Over Y = performance_x / performance_y
    $$
    
    또 X가 Y보다 m% 빠르다고 이야기할 수 있다. 따라서 m = 100 * (1.5 - 1) = 100 * 0.5 = 50%이고, m = 100 * (1/10 - 1/15) / 1/15  = 50% 이다. → 
    
    $$
    m = 100(n-1) = 100(performance_x - performance_y) / performance_y = 100(speedup - 1)
    $$
    
    $$
    n-1 = (perf_x - perf_y) / perf_y
    $$
    
    그리고 유사하게 X의 실행 시간이 Y에 비해 얼마나 줄어들었는지를 보려면 `실행시간의 차이 / y의 실행 시간` 해서 (15 - 10) / 15 = 33% 가 된다.
    
    X에 비해 Y의 실행시간이 얼마나 증가했는지를 보려면 `실행시간의 차이 / x의 실행 시간` 해서 (15 - 10) / 10 = 50% 가 된다.
    
- CPU Clocking
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/89db300d-8150-4867-bdfe-1df43574b939/Untitled.png)
    
    성능에 대한 수식을 이끌어내기 위해 필요한 것들을 알아보자.
    
    먼저 cpu 클럭에 대해 알아보자. 모든 디지털 시스템은 일정하게 동작하는 클럭에 의해 동작하게끔 되어 있다. (constant-rate clock) 그 클럭을 가지고 주파수를 배가시켜서 높은 주파수를 만들어 그걸로 디지털 시스템에 보급한다. 특히 플립플롭에 공급되어 플립플롭의 인풋에 있던 것들이 저장되어 커뮤니케이션 로직에서 입력된 데이터를 통해 일이 수행되고 결과가 나와 다시 출력을 저장하는 플립플롭에 클럭이 들어가 출력된다. 
    
    클럭이 라이징 에지에서 데이터들이 동작하고 동작한 결과들이 그 다음 클럭에서 플립플롭에 저장되고 그에 의해 동작한 것들이 또 다음 클럭에서 동작하고... 를 반복한다.
    
    **clock period** `T` 란 클럭 하나의 기간 = **clock cycle time**을  의미한다. 
    
    클럭에 대해 이야기할 때 주파수, **clock frequency** = **rate** `f` 에 대해서도 이야기한다. 1초에 몇 번 클럭이 사이클을 반복했는지를 의미한다.
    
    $$
    T = 1/f
    $$
    
    4.0GHz 동작한다 = 1초에 4 * 10^9 (40억 개)만큼의 클럭 사이클이 반복된다. 따라서 클럭 하나는 250 picoseconds 가 된다. pico는 10의 -12승을 의미한다. (나노는 10의 -9승이고  -6승이 마이크로, -3승이 밀리이다. )
    
- CPU Time
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/445d3029-5df1-43ba-8d87-474aef8e48fe/Untitled.png)
    
    클럭이 빠르면 빠를수록 좋다. 그래서 cpu를 살 때 3.5GHz와 4.0GHz가 가격이 다르다. 동작 클럭이 다르니까.
    
    CPU Time이란 CPU execution time을 의미한다. CPU Time은 어떠한 프로그램을 실행하는데 클럭 사이클을 몇 개나 필요했는지와 클럭 사이클 하나 도는데 걸리는 시간을 곱하면 나온다. `T = 1/f` , 즉 Clock Cycle Time = 1 / Clock Rate
    
    $$
    CPU Time = CPU Clock Cycles * Clock Cycle Time = CPU Clock Cycles / Clock Rate
    $$
    
    CPU Time은 따라서 클럭 사이클의 개수를 줄일 수 있다면 줄어들 것이다. 하나의 프로그램에 필요한 클럭 개수를 줄여도 줄어들 것이고 보다 빠른 클럭을 사용해도 CPU Time이 줄어들 것이다. 하지만 클럭을 줄이면서 복잡해져 클럭 사이클이 많아져 CPU Time이 줄지 않을 수도 있다. 따라서 하드웨어 설계자들은 Clock rate와 Cycle count 사이에서 적절한 선을 찾는다.
    
    만약 프로세서가 2GHz의 주파수를 가지고 있다면, 1초에 20억 개의 클럭이 뜰 것이다. 각 clock tick마다 하나 이상의 명령어들이 처리되므로 1초에 20억 개의 명령어가 처리된다. 
    
- CPU Time Example
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dde678dc-967e-429c-9006-9824662ac835/Untitled.png)
    
    어떤 컴퓨터 A가 2GHz의 클럭을 가졌고 P에 대한 CPU Time이 10초이다.
    
    컴퓨터 B를 설계하는데 P에 대해 CPU Time을 6초로 하고 싶다면 얼마나 빠른 클럭을 설계해야 할까? 단, 클럭을 빠르게 하면 1.2배의 클럭 사이클을 야기시킨다고 가정한다.
    
    - CPU Time = Clock Cycles / Clock Rate
    - Clock Rate_B = Clock Cycles_B / CPU Time_B = 1.2 * Clock Cycle_A / 6s
    - Clock Cycles_A = CPU Time_A * Clock Rate_A = 10s * 2GHz = 20 * 10^9
    - Clock Rate_B = 1.2 * 20 * 10^9 / 6s = 4GHz
    
- Instruction Count and CPI
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e2b81622-52b6-4fb6-984d-51ffdbf9a204/Untitled.png)
    
    명령어 개수와 CPI에 대해 이야기 해보자.
    
    우리가 프로그램을 실행하는데 필요한 명령어의 개수가 있다. 이는 실제로 실행될 명령어의 개수이다. 프로그램이 아무리 작아도 loop가 있다면 명령어의 개수가 많을 수 있다. 프로그램에 있어서 이러한 명령어의 개수는 프로그램과 명령어 집합(ISA) 과 compiler 에 의해 결정된다. 
    
    그리고 평균 CPI라는 것을 사용할 것이다. CPI는 명령어가 평균 몇 개의 사이클을 사용하는지를 뜻한다.  이는 CPU 하드웨어에 의해 결정된다. 일반적으로 다른 명령어는 다른 CPI를 가진다. 따라서 우리는 Average CPI 를 이용한다. 
    
    CPU Clock Cycles 는 결국 명령어 개수가 몇 개인지, 그리고 하나의 명령어당 몇 개의 사이클이 필요한지에 의해 결정될 것이다. 
    
    $$
    CPU Clock Cycles = Instruction Count * Cycles Per Instruction
    $$
    
    CPU Time은 명령어의 개수 * 평균 CPI * Clock Cycle Time 에 의해 결정될 것이다.
    
    $$
    CPU Time = Instruction Count * CPI * Clock Cycle Time = Instruction Count * CPI / Clock Rate
    $$
    
    CPI 예시로, 같은 명령어 집합 아키텍쳐 (ISA) 라면 명령어의 개수가 같아진다. 이를 I라고 하자. A 시스템은 Cycle Time이 250 ps 이고 평균 CPI가 2.0이다. B 시스템은 Cycle Time이 500 ps 이고 (주파수가 A보다 떨어진다) 평균 CPI가 1.2 이다. 
    
    이 경우 A의 CPU Time은 I * 2.0 * 250 ps = I * 500 ps이고, B의 CPU Time은 I * 1.2 * 500 ps = I * 600 ps 이다.
    
    따라서 A가 B보다 1.2배 빠르다.
    
- CPI in More Detail
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/642f6239-1b27-4dc7-8411-4d9a70d7231b/Untitled.png)
    
    CPI를 더 자세히 살펴보자.
    
    다른 종류의 명령어들은 사이클의 개수가 다르다. 종류 별로 다른 명령어 CPI를 가지고 있기 때문에 Clock Cycle을 계산하려면 CPI_i * Instruction_Count_i 를 모든 종류의 명령어 클래스에 대해 더해 구할 수 있을 것이다. 이를 이용해 CPI (= Clock Cycles / Instruction Count, 전체 사용된 클럭의 사이클의 개수 나누기 전체 사용된 명령어의 개수) 를 구하려면 CPI_i * Instruction Count_i / Instruction Count 를 모든 종류의 명령어 클래스에 더해야 한다. (i 클래스 명령어가 전체 명령어에 대해 얼마나 자주 나타났는지를 구해 CPI_i 에 곱한 값을 모두 더한다)
    
    CPI에 대한 예시로 어떤 프로그램이 컴파일될 때 명령어의 클래스가 A, B, C 가 있는데 1, 2, 3 이라는 CPI를 가지고 있고 Sequence 1로 컴파일하면 명령어(IC) 가 5개가 나오는데 이는 각각 클래스별로 세어 보면 2개, 1개, 2개이다. Sequence 2로 컴파일하면 6개의 명령어가 나오는데 4개, 1개, 1개 씩 나왔다.
    
    Sequence 1의 경우, Clock cycle은 2 * 1 + 1 * 2 + 2 * 3 = 10 이고, Avg CPI 는 10 / 5 = 2.0 이다.
    
    Sequence 2 의 경우, Clock cycle은 4 * 1 + 1 * 2 + 1 * 3 = 9 이고, Avg CPI 는 9 / 6 = 1.5 이다.
    
- CPU Performance Summary
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e2531f3e-cc54-4267-b959-ba66a069ae27/Untitled.png)
    
    따라서 최종적으로 CPU Performance에 대한 정리식은 다음과 같다. CPU Time이란 `하나의 프로그램을 수행하기 위해 필요한 명령어들의 개수` 곱하기 `하나의 명령어를 처리하는데 걸리는 평균 클럭 사이클의 수(= 평균 CPI)` 곱하기 `하나의 클럭이 실행하는데 걸리는 시간 (= Clock Cycle Time)` 이다. 따라서 컴퓨터 시스템의 CPU Time을 줄이기 위해서는 하나의 프로그램을 돌리는데 필요한 명령어의 수를 줄여야 하고 하나의 명령어를 처리하는데 필요한 클럭 사이클의 개수를 줄여야 하고, 클럭 사이클 타임을 줄여야한다, 즉 동작 주파수를 올려야 한다.
    
    CPU Time은 명령어의 개수 * 평균 CPI * Clock Cycle Time 
    
    $$
    CPU Time = Instructions / Program * Clock Cycles / Instruction * Seconds / Clock Cycle
    $$
    
    CPU Time이란 프로그램이 CPU를 이용해서 동작하는 순수한 Time을 의미한다. 이러한 CPU Time은 하드웨어 관점에서 크게 세 가지로 구성되어있다. 
    
    1. 프로그램을 위해 실행되어야 하는 명령어의 개수
    2. 명령어당 필요한 평균 클럭 사이클의 수 = 평균 CPI
    3. 하나의 클럭 사이클을 실행하는데 걸리는 시간 = Clock Cycle Time = Clock Period
    
    따라서 성능을 높이려면 이 각각의 값들이 작아져야 한다.
    
    그렇다면 위 세 개의 팩터들에 영향을 끼치는 것은 무엇이 있을까?
    
    1. 알고리즘을 잘 짜면 실행되어야 하는 명령어의 개수가 줄어들 것이다.
    2. 프로그래밍 언어에 따라 실행되는 명령어의 개수와 CPI가 차이가 난다.
    3. 컴파일러도 좋은 컴파일러, 나쁜 컴파일러가 있어서 명령어의 개수를 줄일 수 있고 CPI에 영향을 줄 수 있다.
    4. 명령어 집합 아키텍쳐란 프로세서 타입을 말한다. PC에서는 인텔 아키텍쳐를 사용하고 있다. AMD와 intel은 같은 아키텍쳐를 공유한다. 스마트폰에서는 ARM 아키텍쳐를 사용한다. 이 경우에는 아키텍쳐의 종류가 다르다. 따라서 명령어의 개수, CPI가 달라질 수 있다. ARM은 RISC 프로세서로, 하나의 명령을 처리할 때 하나의 사이클 또는 그 이하의 사이클을 사용한다. 그리고 RISC 의 경우, 그를 구현하는 하드웨어도 간단해 Clock Rate에 영향을 미칠 수도 있다. 
    5. 내부 아키텍쳐를 어떻게 구현하느냐 (인텔? AMD?)에 따라 CPI와 Clock Rate 가 결정된다. 같은 명령어의 개수가 나오지만 한 명령어를 실행할 때 구현에 따라 CPI가 달라질 수 있고 Clock Rate이 달라질 수 있다.
    
    사용할 CPU가 정해져있고 개발 환경도 정해져 있기 때문에 아키텍쳐, ISA, Compiler fixed 등을 정해야 한다. 또 다른 노력으로 알고리즘과  프로그래밍 언어에 집중하는 방법도 있다.
    
- CPU Performance Example
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8a3a51d5-9ee9-424d-a5c0-b691ca4347a1/Untitled.png)
    
    CPU 성능의 예에 대해 살펴보자.
    
    CPU Time은 프로그램을 실행하는데 필요한 명령어의 개수,  그리고 각 명령어당 평균 클럭 사이클의 개수 = 평균 CPI, 하나의 클럭이 걸리는 시간 = 클럭 사이클 타임 이 세 개의 factor로 이루어져 있다.
    
    클럭 속도(= Clock Rate)는 클럭 사이클 타임과 연관되어 있고, CPI는 평균 CPI(= 평균 클럭 사이클의 개수) 와 연관되어 있다.
    
    1. 어떤 프로세서의 성능이 가장 좋은가?
        
        `초당 명령어의 개수 = 명령어의 개수 / 시간 (= IC/Time)`
        
        [CPU Time = 명령어의 개수 * 평균 CPI * Clock Cycle Time](https://www.notion.so/1-overview-f28065feb8c24e9db41bed3f070e9763) 이고,  [Clock Cycle Time = 1 / Clock Rate](https://www.notion.so/1-overview-f28065feb8c24e9db41bed3f070e9763)  이므로
        
        `명령어의 개수 / CPU Time = 1 / (CPI * Clock Cycle Time) = Clock Rate / CPI`
        
    2. 어떤 프로그램을 수행하는데 모두 10초가 걸렸을 때 사이클의 수와 명령어의 개수를 구하라.
        
        `사이클 수 = 명령어 개수 * 평균 CPI` 
        
        이 때 명령어 개수 (= IC) = Time * Clock Rate / CPI 이므로, 
        
        `Cycles = Time * Clock Rate / CPI * CPI = Time * Clock Rate` 
        
    3. 실행 시간을 30% 단축시키고 싶을 때 CPI는 20% 증가한다 가정했을 때 클럭 속도는?
        
        [Clock Cycle Time = 1 / Clock Rate](https://www.notion.so/1-overview-f28065feb8c24e9db41bed3f070e9763) 이고 [CPI Time = IC * CPI * Clock Cycle Time](https://www.notion.so/1-overview-f28065feb8c24e9db41bed3f070e9763) 이므로
        
        `Clock Rate (= f) = IC * CPI / Time` 이다.
        

## 1주 3차시 - 파워와 멀티프로세서

CAU01-intro\PowerAndMultiprocessor(18p)

- Power Trends
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/baa7edb2-382e-42be-9b20-0867e41ebbaa/Untitled.png)
    
    intel 기준 파워 트렌드이다.
    
    IBM PC가 처음 나왔을 때 XT라는 것이 나왔다. 이는 8088이라는 프로세서로 만들어졋었다. 나중에 IBM PC AT (50286)가 1982년에 나왔다. 그리고 1985년부터 32bit architecture가 되었다. 
    
    파란 선은 동작 주파수. IBM PC AT 때만 해도 동작 주파수가 12.5 MHz였다. 그리고 1985년엔 16MHz, ... 그리고 펜티움이 등장하면서 66, 200,  이렇게 크게 성장했다. 그리고 펜티움 4가 등장하면서 2000MHz, 즉 2GHz 를 찍고 그 다음으로는 3GHz 를 넘어선다. 
    
    검은 선은 전력 소모. 80286 때만 해도 3.3 watts를 사용했다. 그리고 천천히 증가하다가 클럭 속도가 증가하는 속도에 따라 높아지다가 저전력이라는 이슈가 대두되면서 저전력을 위한 기술들이 개발되면서 전력 소모가 떨어지기 시작했다. Pentium 4 (Prescott) 가 있고 Core 2 (Kensfield)  가 있었는데, Pentium 4가 집적도는 더 높았지만 Core 2는 내부 구조에서 pipeline을 더 간단하게 해서 동작을 쉽게 하도록 만들었다. 
    
    $$
    Power = Capcitive load * Voltage^2 * Frequency
    $$
    
    파워는 칩 내의 load 성분 * 동작 전압의 제곱 * 동작 주파수 로 만들어져 있다. 
    
    전력을 떨어뜨리려면 주파수는 유지시키고 전압은 낮춰야 한다. 
    
    앞에서 봤듯 메모리에서 DDR에서 진화하면서 동작 전압이 낮아지는 이유는 동작 주파수가 낮아지고 있으므로 전압을 떨어뜨려 파워를 낮추려고 하고 있기 때문이다. 
    
    따라서 power는 한계에 부딪혔다. 주파수도 더 높일 수 없고 파워도 더 높일 수 없다.
    
    지금 모바일 장치에서는 그것이 더 심해졌다.
    
- Reducing Power
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/baa33526-ec50-47c7-bae7-2ebafc63aba8/Untitled.png)
    
    새로운 CPU가 기존 CPU보다 더 잘 만들어 capacitive load를 85%로 낮추고 전압도 15% 낮아지고 주파수도 15% 낮아졌다면 전력 면에서 얼마나 좋아졌을까?
    
    Power_new / Power_old = (C_old * 0.85) * (V_old * 0.85)^2 * *(F_old * 0.85) / C_old * V_old^2 * F_old = 0.84^4 = 0.52
    
    계속 전압을 줄여 왔는데 더 이상 전압을 줄이기가 어려워 전력을 소모할 때 열이 나서 열을 제거하기가 더 어려워지는 한계에 부딪힌 현실. 하지만 성능을 높이기 위해 주파수를 올려야 하는데 우리는 어떻게 성능을 향상시킬 수 있을까?
    
    → 해답: 유니프로세서가 아닌 멀티프로세서로 가자!
    
- Uniprocessor Performance
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/76ec097c-12d7-42b2-a6eb-d88689c09161/Untitled.png)
    
    코어 하나만 있는 상황에서 1년에 25% , 22% 씩 성능이 높아져 오다가 벽에 부딪침
    
    → 멀티프로세서 개념을 만듬
    
- Multiprocessors
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/111778f3-f407-41d9-81ed-86db196161c5/Untitled.png)
    
    칩의 집적도는 계속 높아졌고 지금도 집적도를 높이고자 노력하고 있다.
    
    TSMC, SAMSUNG 에 시스템 반도체를 만들기 위한 foundary를 가지고 있다. TSMC의 경우 5ns의 제품을 생산하고 있고 SAMSUNG은 간헐적으로 5ns 인데 공장의 성능?이 좋지는 않다. TSMC, SAMSUNG의 목표는 3ns 이다.
    
    3ns란, 웨이퍼 위에 회로를 그리는데 선폭 자체를 3ns로 그릴 수있다는 것이다. 그렇게 된다면 훨씬 더 많은 것들이 들어갈 수 있어지면서 칩의 집적도가 높아지고 더 고성능의 칩을 만들 수 있게 될 것이다. 그것을 uniprocessor로 하지 않고 multi core로 할 것이다.
    
    cpu 자체를 여러 개를 두도록 해서 듀얼 코어, 쿼드 코어 등의 이름으로 나온다. 이 기술은 원래 병렬 처리 시스템의 기술들을 가져와서 원칙으로 만들어낸 것이다. 
    
    cpu가 여러 개가 되면 명시적으로 병렬 프로그래밍을 해야 한다. 다행히 pc의 경우 부팅이 끝나고 윈도우가 뜨면 시스템 서비스를 위한 프로세서들도 있고 시작 프로그램도 있고 해서 코어가 여러개라도 각각을 분배해서 맡길 수 있다.
    
    하지만 결국 고성능의 프로그램을 돌리기 위해서는 parallel programming을 해야 한다. 하지만 사람은 이처럼 생각하지 못하기 때문에 어렵다. 
    
    instruction level에서의 parallelism은 동시에 여러 개의 명령어를 실행시키는 기법이다. 프로그래머는 이것을 몰라도 된다. 하지만 core level에서 하려면 명시적으로 parallel programming을 해야 하다. (그래서 어렵다)  
    
    parallel programming의 어려움
    
    1. 성능을 향상시키기 위해 프로그래밍하기 어려움
    2. 로드 밸런싱 - 균등하게 일을 cpu들에게 분배
    3. cpu 간 통신 최적화, 공유되는 자원에 대한 동기화
- Amdahl’s Law
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/33c05919-38fc-4a2f-9b50-e701b37af550/Untitled.png)
    
    암달의 법칙에 의하면 병렬로 동작시킬 수 있는 프로그램이 많아도 실제 프로그램이 어떤 상태인지에 따라 제한을 받는다는 법칙. 전체 성능이란 컴퓨터의 개선 가능한 부분에 의해 영향을 받는다는 법칙.
    
    기존에 100 시간이 걸리는 것이 있었는데 이 중에 개선시켜서 개선된는 부분과 개선안되는 부분을 나눠 개선안되는 부분은 시간을 다 사용하고 개선되는 부분은 개선할 수 있다. 개선의 팩터, improvement factor (p) 가 1이면 개선이 안되고 2이면 원래 시간이 반으로 줄어들어 개선이 이루어진다. 이 개선 척도가 얼머나 되는지를 가지고 T_improved가 얼마가 되는지를 결정할 수 있다. 
    
    전체 수행 시간이 100초이고 이 중 80초가 곱하기 연산에 사용되고 나머지는 어쩔 수 없는 시간이라 가정하자. 이를 개선해서 5배의 성능 향상을 시키고 싶다면 (20초에 수행시키려면) 얼마나 개선시켜야 할까?
    
    → 20 = 80 / n + 20 
    
    → n = 무한대가 되어야 하므로 불가능하다 (한계 있음)
    
    Speed up (S) 란 기존에 비해 얼마나 빠르게 처리했는지를 보여주는 것이다. 기존 동작 시간은 T_affected + T_unaffected 이다. 개선된 시간은 T_affected/p + T_unaffected이다.
    
    S : Speed up, f : fractional unaffected, p: speedup of the rest 라고 할 때, (1 - f == affected)
    
    $$
    S = 1 / (f + (1-f) / p) <= min(p, 1/f)
    $$
    
    이상적으로는 전체가 affected였을 경우 최대 speed up 은 p 이고, 그렇지 않은 경우 affected인 쪽이 0에 가깝게 개선되었을 때 최대 1 / f 에 수렴하는 speed up을 가질 수 있다.
    
- Amdahl’s Law Example
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/982e451e-eb84-4e0f-b12c-752e3c207b04/Untitled.png)
    
    1. FP 명령어의 수행 시간이 20% 줄면 전체 수행 시간은?
        
        fp 명령어 시간 T_newfp = 70 * 0.8 = 56
        
        → T_new = 234초 → 전체적으로 5.6% 감소.
        
    2. 전체 수행 시간이 20% 줄려면 INT 명령어 수행 시간은 얼마나 줄어야 할까?
        
        → T_new = 200초가 되게끔 해야 한다. 따라서 INT명령어 수행 시간을 50초 감소시켜 5초의 수행 시간을 가지도록 해야 한다. → 91% 감소되어야 한다.
        
    3. 분기 명령어 시간의 감축만으로 전체 수행 시간을 20% 줄일 수 있나?
        
        전체 시간이 20% 줄려면 50초 감소해야 하지만 BR 명령어는 40초 이므로 불가능하다.
        
    
    이 예시에서는 LS 명령어의 수행 시간을 줄이려 하는 것이 효과가 크다.
    
- Recent Evolution of Computer Architecture
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fed85c92-3f98-4a1b-9050-9ab7c2cd89c8/Untitled.png)
    
    유니프로세서 → 멀티프로세서로 발전했고 새로운 응용 분야들이 나오고 있다.
    
    AI가 나오면서 AI에 필요한 연산들을 많이하게 되면서 GPU를 많이 사용함. GPU는 원래 그래픽 처리를 위한 프로세서였는데 요즘은 비트코인 채굴에도 많이 사용된다. 병렬 문제 푸는데 적합한 구조여서 matrix, vector 연산을 빠르게 병렬적으로 풀 수 있기 때문이다. 그래서 GPU가 그래픽 처리 뿐만 아니라 General Purpose 형태로 변한다. (GPGPU) 그래서 AI 연산도 matrix, vector 연산이 많아 AI에서도 학습시키고 추론하는데 많이 사용된다.
    
    AI 발전에 따른 변화 중 또 한가지가 AI 전용 프로세서가 만들어지고 있다는 것이다. 구글의 TPU (Tensor Processing Unit), 마이크로소프트의 Brainwave 등의 AI 칩들이 있다.
    
- SPEC CPU Benchmark
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4293fdf9-a0ff-4a0a-8e4b-4dd3b606d7f0/Untitled.png)
    
    어떠한 시스템이 어떤 성능을 가지는지 알아보는 가장 좋은 방법은 실제 그것이 동작해야 하는 환경에서 돌려보는 것이다. 이렇게 실제 컴퓨터에 load를 걸어서 돌려보는 프로그램들의 집합을 Workload라고 한다. 실제 프로그램으로 workload를 만들어 사용하고 있다. 어떤 프로그램을 사용하는지, input은 뭐고 빈도는 어떻게 되는지를 가지고 만들어야 한다.
    
    Benchmark란 컴퓨터 성능을 비교하기 위해서 골라내어지는 프로그램들이다. 실제 actual workload 와 유사하다. 
    
    SPEC 은 Standard Performance Evaluation Corp에서 만든 것으로, cpu 성능을 확인하기 위해 SPEC을 만들어 계속 업데이트시켜왔다. CPU의 경우 가장 최신 버전은 SPEC CPU2017 이다. 
    
    CPU 뿐만 아니라 java 성능, 웹 성능 들을 확인하기 위한 Benchmark 프로그램들이 많이 만들어졌다. 
    
    SPEC CPU2017 의 경우 프로그램들을 실행시켜 실제 시행된 시간을 구한다. CPU performance에 중점을 두고 있기에 i/o 들은 무시할 만한 프로그램을 사용한다. 어떤 reference machine이 있는데 상대적인 성능을 거기에 normalize한다. 그 다음에 각각 성능에 대한 실행 타임으로  geometric mean을 구해 수치를 제공해준다. 이것이 SPEC Benchmark 지수가 된다. 
    
    이 또한 Integer 연산을 하는 형태의 것과 Floating Point 연산을 하는 형태의 것으로 나뉘어져 있다.
    
- performance Comparing
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7536d61b-0f60-4fe9-b1eb-0016eaf57634/Untitled.png)
    
    기하 평균이 성능 비교에 유용하다는 것을 예를 들어 살펴보겠다.
    
    프로그램 세 개를 컴퓨터 X에서 돌리는 시간과 컴퓨터 Y에서 돌리는 시간을 구했다.
    
    A는 X에서 20, Y에서 200 이라는 시간이 걸렸다. 이 때 X 기준으로 Y의 speed up을 구해보면 0.1 이다. (20 / 200 = 0.1)  Y 기준으로 X의 speed up을 구해보면 10 이다. (200 / 20 = 10) 
    
    B에서는 X 기준으로 Y의 speed up이 10 이고, Y 기준으로 X의 speed up 이 0.1이다.
    
    C에서도 X 기준으로 Y의 speed up이 10 이고, Y 기준으로 X의 speed up 이 0.1이다.
    
    산술 평균으로 X 기준으로 Y의 speed up 을 구하면 6.7 이 나온다. 산술 평균으로 Y 기준으로 X의 speed up 을 구하면 3.4 이다. 하지만 이것만 가지고는 어느 것이 더 좋은지 명확하게 따지기 어렵다. Y에 비해 X 가 좋고 X 에 비해 Y 가 좋다는 뜻이 되기 때문이다.
    
    반면에 수행 시간을 모두 더한 값인 2520, 450 을 기준으로 speed up을 구하면 X 기준으로 Y의 speed up 이 5.6, Y를 기준으로 X의 speed up 이 0.18 이다. 이러하면 명확하게 X와 Y의 좋고 나쁨이 가려진다. 하지만 한 프로그램의 실행시간에 의해서 과도하게 그 값들이 결정될 수 있다는 단점이 있다.
    
    이번에는 각 프로그램의 speed up 에 대한 기하 평균을 예로 들어보겠다. 그렇게 구해보면 X 기준으로 Y의 speed up이 2.154이고 Y 기준으로 X의 speed up이 0.464 이다. 이것을 보면 어느 것이 더 성능이 좋고 나쁘고를 구분할 수 있다. 상대적인 성능 비교가 가능하다. 또한 한 프로그램이 과도하게 시간을 많이 걸리게 할 경우 그것에 대한 영향이 끼치는 것을 방지할 수 있다. 
    
    따라서 SPEC에서는 성능 측정할 때 기하 평균으로 나타내게끔 하고 있다.
    
- CIT2006 for Intel Core i7 920
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5244de77-68c1-42cd-a00e-bac6efed424f/Untitled.png)
    
    여러가지 프로그램들을 나열해 실제로 돌려서 명령어의 개수, CPI, Clock Cycle TIme을 구하고 Execution Time, Reference Time (기준이 되는 컴퓨터의 수행시간) 를 구해서 Speed up 을 구할 수 있다. speed up (SPECratio) 을 구해서 이것의 geometric mean 을 구해 SPEC 값으로 기존 컴퓨터보다 얼마나 좋은지를 확인할 수 있다.
    
- Performance Benchmark Sample
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/aa2ad27d-f63b-4de3-ab88-2e94dc3d6d92/Untitled.png)
    
    조금 지난 자료가 더 좋은 것들이 나왔지만 AMD가 상위를 차지하고 있다. 원래는 intel이 최고였는데 변함. 
    
    그래픽 카드도 성능이 더 좋은 것들이 더 나왔음.
    
    내 pc 성능이 어느정도인지 찾아보자.
    
- Concluding Remarks
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b2b8412b-ffb8-4c18-b274-f74ca7481a81/Untitled.png)
    
    컴퓨터가 발전해오면서 성능 / 비용 이 많이 발전했다. 반도체  기술의 발전이 주요 요인이다. (반도체는 그만큼 중요).
    
    계층 구조를 가지고 있어 접근하기 쉽다.
    
    명령어 집합 아키텍쳐 MIPS ISA 를 다루게 될 것. 
    
    실행 시간이 가장 좋은 성능 측정 척도이다.
    
    파워는 limiting factor 로 동작하게 되어 성능을 높이기 위해 병렬성을 추구해왔다
    
- Future Topics
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/99c35a4a-e237-4d64-b536-e8a82b128daf/Untitled.png)
    
    Computer Arithmetic - ALU , ...
    
    secondary memory - SSD, HDD
    
    virtual memory - secondary memory 와 main memory 사이에 virtual memory가 어떻게 구현되는지 알아볼 것. 이 virtual memory는 운영체제의 영역이기도 하다.
